model: "llama"
version: "1.0"
training:
  epochs: 5
  batch_size: 16
  learning_rate: 5e-5
  weight_decay: 0.01
  gradient_accumulation_steps: 2
  max_grad_norm: 1.0
  warmup_steps: 500
  logging_steps: 100
  save_steps: 1000
  evaluation_strategy: "steps"
  eval_steps: 500
  save_total_limit: 2
data:
  train_file: "../data/splits/train.jsonl"
  validation_file: "../data/splits/val.jsonl"
  test_file: "../data/splits/test.jsonl"
  preprocessing:
    max_seq_length: 512
    padding: "max_length"
    truncation: true
hyperparameters:
  search:
    method: "grid"
    parameters:
      learning_rate: [1e-5, 5e-5, 1e-4]
      batch_size: [8, 16, 32]
      weight_decay: [0.0, 0.01, 0.1]
output:
  model_dir: "../models/llama_finetuned"
  metrics_dir: "../experiments/experiment_001/results.csv"